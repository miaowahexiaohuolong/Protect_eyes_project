import os
import pickle
import faiss
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from mlx_lm import load, generate

# ================= é…ç½®è°ƒæ•´ =================
# 1. è·¯å¾„ (ä¿æŒä¸å˜)
KB_DIR = "my_knowledge_base"
INDEX_FILE = os.path.join(KB_DIR, "health.index")
META_FILE = os.path.join(KB_DIR, "health.pkl")

# 2. æ¨¡å‹ (ä¿æŒä¸å˜)
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
LLM_MODEL = "mlx-community/Qwen2.5-14B-Instruct-4bit"
ADAPTER_PATH = "deepseek_clear_Data/my_adapters_14b"

# 3. æ£€ç´¢å‚æ•° (ä¿®æ”¹)
RETRIEVAL_TOP_K = 10  # å‘é‡å–10ä¸ªï¼Œå…³é”®è¯å–10ä¸ª
# ===========================================

class HybridRetriever:
    def __init__(self):
        print("\nğŸ“š [Init] Initializing hybrid retrieval (full)...")
        if not os.path.exists(INDEX_FILE):
            raise FileNotFoundError("Knowledge base files not found")

        # 1. åŠ è½½æ–‡æ¡£
        with open(META_FILE, 'rb') as f:
            self.documents = pickle.load(f)
            
        # 2. åŠ è½½å‘é‡æ£€ç´¢ (Faiss)
        print("   -> Loading Faiss...")
        self.index = faiss.read_index(INDEX_FILE)
        self.embedder = SentenceTransformer(EMBEDDING_MODEL)
        
        # 3. åŠ è½½å…³é”®è¯æ£€ç´¢ (BM25)
        print("   -> Building BM25 index...")
        corpus_tokens = [self._tokenize(doc.get('title', '') + " " + doc.get('abstract', '')) for doc in self.documents]
        self.bm25 = BM25Okapi(corpus_tokens)
        
    def _tokenize(self, text):
        return text.lower().split()

    def search(self, query):
        """
        ä¿®æ”¹ç‰ˆï¼šä¸è¿›è¡Œæ’åºï¼Œç›´æ¥åˆå¹¶ä¸¤ä¸ªæ£€ç´¢æºçš„æ‰€æœ‰ç»“æœå¹¶å»é‡
        """
        results = []
        seen_ids = set() # ç”¨äºå»é‡ ID

        # --- 1. å‘é‡æ£€ç´¢ (Vector Search) ---
        query_vec = self.embedder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_vec)
        _, vector_indices = self.index.search(query_vec, RETRIEVAL_TOP_K)
        vector_ids = vector_indices[0] 
        
        # æ”¶é›†å‘é‡ç»“æœ
        for doc_id in vector_ids:
            if doc_id == -1: continue
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                results.append({
                    "source": "Vector", # æ ‡è®°æ¥æºæ–¹ä¾¿è°ƒè¯•
                    "id": doc_id,
                    "title": self.documents[doc_id].get('title', ''),
                    "abstract": self.documents[doc_id].get('abstract', '')
                })

        # --- 2. å…³é”®è¯æ£€ç´¢ (Keyword Search) ---
        tokenized_query = self._tokenize(query)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        keyword_ids = np.argsort(bm25_scores)[::-1][:RETRIEVAL_TOP_K]
        
        # æ”¶é›†å…³é”®è¯ç»“æœ (ä»…æ·»åŠ ä¹‹å‰æ²¡è§è¿‡çš„)
        for doc_id in keyword_ids:
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                results.append({
                    "source": "Keyword",
                    "id": doc_id,
                    "title": self.documents[doc_id].get('title', ''),
                    "abstract": self.documents[doc_id].get('abstract', '')
                })
        
        # æ­¤æ—¶ results åŒ…å«äº† Vector Top 10 + Keyword Top 10 (å»é‡å)
        # æ€»æ•°åœ¨ 10 åˆ° 20 ä¹‹é—´
        
        # é‡æ–°ä¸ºæœ€ç»ˆåˆ—è¡¨ç¼–å·
        for i, res in enumerate(results):
            res['rank'] = i + 1
            
        return results

class LightRAGBot:
    def __init__(self, model, tokenizer):
        self.retriever = HybridRetriever()
        self.model = model
        self.tokenizer = tokenizer

    def verify(self, query):
        print(f"\nğŸ” [Step 1] Full retrieval (Vector + Keyword)...")
        evidence = self.retriever.search(query)
        
        print(f"   -> Retrieved {len(evidence)} evidence entries (no truncation):")
        for doc in evidence:
            source_tag = "[V]" if doc.get("source") == "Vector" else "[K]"
            print(f"      {source_tag} [{doc['rank']}] {doc['title'][:50]}...")
            
        context_str = ""
        if not evidence:
            context_str = "ã€System Noticeã€‘: No relevant literature in the database."
        else:
            for doc in evidence:
                context_str += f"--- Evidence [{doc['rank']}] ---\nTitle: {doc['title']}\nAbstract: {doc['abstract']}\n\n"
            
        # English prompt enforcing natural language output
        prompt = f"""
## Strict Instructions
Do not output JSON, dictionaries, or Python lists. Write in clear, natural English as a professional clinician.

## Task
Using the following Evidence, verify the truthfulness of the User Claim.

User Claim: {query}
Evidence:
{context_str}

## Output format (Markdown)

### 1. Core Verdict
*(Write only one sentence, starting with ğŸ”´Do not buy / ğŸŸ¡Consider with caution / ğŸŸ¢Reasonable to buy)*

### 2. Detailed Analysis
*(Use unordered list `*`. No quotes, no brackets.)*
* About the claim: [content]
* Scientific evidence: [content]
* Contradictions: [content]

### 3. Final Recommendation
*(Write a short paragraph. No JSON.)*
"""
        print(f"\nğŸ“ [Step 2] AI generating verification report...\n")
        print("-" * 60)
        
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        # æ³¨æ„ï¼šå› ä¸ºè¿™é‡Œè¾“å…¥äº†æœ€å¤š20ç¯‡æ–‡æ¡£ï¼ŒPromptä¼šå¾ˆé•¿ï¼Œç”Ÿæˆæ—¶è¯·ç¡®ä¿æ˜¾å­˜è¶³å¤Ÿ
        generate(self.model, self.tokenizer, prompt=text, max_tokens=2048, verbose=True)
        print("-" * 60)

if __name__ == "__main__":
    # åœ¨è¿™é‡ŒåŠ è½½æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡é‡å¯
    print("â³ Loading model...")
    model, tokenizer = load(LLM_MODEL, adapter_path=ADAPTER_PATH)
    
    bot = LightRAGBot(model, tokenizer)
    
    while True:
        q = input("\nEnter ad claim (q to quit): ").strip()
        if q.lower() == 'q': break
        if not q: continue
        bot.verify(q)